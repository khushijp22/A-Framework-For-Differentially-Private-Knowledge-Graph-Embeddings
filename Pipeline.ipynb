{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of Project Structure\n",
        "\n"
      ],
      "metadata": {
        "id": "gEk98rQnWn1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project implements a knowledge graph embedding model using TransE. The code is modularized into three key files:\n",
        "\n",
        "- `data.py`: Handles dataset loading, generate triples and splitting into unrestricted and confidential subsets based on a configurable `confidential_ratio`. It also generates positive/negative samples and prints relevant statistics.\n",
        "- `model.py`: Defines the TransE model class. It initializes entity and relation embeddings, computes distances between triples, performs forward passes, and manages model parameters.\n",
        "- `trainer.py`: Contains the training logic. It trains the model using the configured parameters and privacy settings and logs the performance metrics.\n"
      ],
      "metadata": {
        "id": "5l91Myc_bmW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X927jl4KWD65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook uses:\n",
        "- `pykeen`: For working with knowledge graph embeddings.\n",
        "- `opacus`: To enable differential privacy during training."
      ],
      "metadata": {
        "id": "2deG7m3jbpM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pykeen==1.10.1 class-resolver==0.3.10"
      ],
      "metadata": {
        "id": "7VIN1Gfu0OGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install opacus"
      ],
      "metadata": {
        "id": "DGWPt3Kg1S9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define data.py: Knowledge Graph Data Handler\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S7n2E_9iWRPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates `KGDataHandler` class, which is responsible for:\n",
        "\n",
        "- Loading training, validation, and test triples from files.\n",
        "- Initializing entity and relation mappings using PyKEEN.\n",
        "- Splitting training data into *confidential* and *unrestricted* subsets based on a given ratio.\n",
        "- Generating both positive and negative samples for training.\n",
        "- Providing key dataset statistics for transparency and debugging."
      ],
      "metadata": {
        "id": "-cy2zQhobvoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data.py\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from typing import List\n",
        "from pykeen.datasets import PathDataset\n",
        "from typing import List, Tuple, Dict, Set, Optional\n",
        "\n",
        "class KGDataHandler:\n",
        "    \"\"\"Knowledge Graph Data Handler\"\"\"\n",
        "\n",
        "    def __init__(self, fb_path: str, confidential_ratio: float = 0.3):\n",
        "        \"\"\"\n",
        "        Initialize the KG data handler\n",
        "\n",
        "        Args:\n",
        "            fb_path: Path to the dataset files\n",
        "            confidential_ratio: Fraction of training data that requires privacy protection\n",
        "        \"\"\"\n",
        "        self.fb_path = fb_path\n",
        "        self.confidential_ratio = confidential_ratio\n",
        "\n",
        "        self.train_data = self.load_triplets(os.path.join(fb_path, 'train.txt'))\n",
        "        self.valid_data = self.load_triplets(os.path.join(fb_path, 'valid.txt'))\n",
        "        self.test_data = self.load_triplets(os.path.join(fb_path, 'test.txt'))\n",
        "\n",
        "        self.train_path = os.path.join(fb_path, 'train.txt')\n",
        "        self.valid_path = os.path.join(fb_path, 'valid.txt')\n",
        "        self.test_path = os.path.join(fb_path, 'test.txt')\n",
        "\n",
        "        self.dataset = PathDataset(\n",
        "            training_path=self.train_path,\n",
        "            testing_path=self.test_path,\n",
        "            validation_path=self.valid_path\n",
        "        )\n",
        "\n",
        "        self._prepare_data()\n",
        "\n",
        "    def load_triplets(self, file_path: str) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Load triplets from file\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the file\n",
        "\n",
        "        Returns:\n",
        "            List of triplets [head, relation, tail]\n",
        "        \"\"\"\n",
        "        with open(file_path, 'r') as f:\n",
        "            triplets = [line.strip().split('\\t') for line in f]\n",
        "        return triplets\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"Prepare data for training\"\"\"\n",
        "        # Create entity and relation dictionaries\n",
        "        self.entities = set()\n",
        "        self.relations = set()\n",
        "        self.entities_t = set()\n",
        "        self.relations_t = set()\n",
        "\n",
        "        for h, r, t in self.train_data:\n",
        "            self.entities.add(h)\n",
        "            self.entities.add(t)\n",
        "            self.relations.add(r)\n",
        "\n",
        "        for h, r, t in self.test_data:\n",
        "            self.entities_t.add(h)\n",
        "            self.entities_t.add(t)\n",
        "            self.relations_t.add(r)\n",
        "\n",
        "        # Get mappings from dataset\n",
        "        self.entity_to_id = self.dataset.training.entity_to_id\n",
        "        self.relation_to_id = self.dataset.training.relation_to_id\n",
        "        self.entity_to_id_t = self.dataset.testing.entity_to_id\n",
        "        self.relation_to_id_t = self.dataset.testing.relation_to_id\n",
        "\n",
        "        self.entity_count = len(self.entity_to_id)\n",
        "        self.relation_count = len(self.relation_to_id)\n",
        "\n",
        "        # Convert string triples to ID triples\n",
        "        self.train_triples = [\n",
        "            (self.entity_to_id[h], self.relation_to_id[r], self.entity_to_id[t])\n",
        "            for h, r, t in self.train_data\n",
        "        ]\n",
        "\n",
        "        self.test_triples = [\n",
        "            (self.entity_to_id_t[h], self.relation_to_id_t[r], self.entity_to_id_t[t])\n",
        "            for h, r, t in self.test_data\n",
        "            if (h in self.entity_to_id and r in self.relation_to_id and t in self.entity_to_id)\n",
        "        ]\n",
        "\n",
        "        # Split training data into confidential and unrestricted\n",
        "        split_point_conf = int(len(self.train_triples) * self.confidential_ratio)\n",
        "        self.confidential_triples = self.train_triples[:split_point_conf]\n",
        "        self.unrestricted_triples = self.train_triples[split_point_conf:]\n",
        "\n",
        "        # All triples for filtered evaluation\n",
        "        self.all_triples = self.train_triples + self.test_triples\n",
        "\n",
        "    def print_data_stats(self):\n",
        "        \"\"\"Print statistics about the data\"\"\"\n",
        "        print(\"--------------------------------------------------------------------------------\")\n",
        "        print(f\"Total entities: {self.entity_count}\")\n",
        "        print(\"--------------------------------------------------------------------------------\")\n",
        "        print(f\"Total relations: {self.relation_count}\")\n",
        "        print(\"--------------------------------------------------------------------------------\")\n",
        "        print(f\"Confidential triples: {len(self.confidential_triples)}\")\n",
        "        print(\"--------------------------------------------------------------------------------\")\n",
        "        print(f\"Unrestricted triples: {len(self.unrestricted_triples)}\")\n",
        "        print(\"--------------------------------------------------------------------------------\")\n",
        "        print(f\"Testing triples: {len(self.test_triples)}\")\n",
        "\n",
        "    def get_positive_and_negative_samples(self, triples, batch_size, neg_ratio=10, entity_count=None):\n",
        "        \"\"\"\n",
        "        Get positive samples and corresponding negative samples\n",
        "\n",
        "        Args:\n",
        "            triples: List of triples [(head, relation, tail), ...]\n",
        "            batch_size: Number of positive samples\n",
        "            neg_ratio: Number of negative samples per positive sample\n",
        "            entity_count: Number of entities in KG\n",
        "\n",
        "        Returns:\n",
        "            Positive and negative samples as tensors\n",
        "        \"\"\"\n",
        "        if entity_count is None:\n",
        "            entity_count = self.entity_count\n",
        "\n",
        "        if len(triples) > batch_size:\n",
        "            batch_indices = random.sample(range(len(triples)), batch_size)\n",
        "            batch_triples = [triples[i] for i in batch_indices]\n",
        "        else:\n",
        "            batch_triples = triples\n",
        "\n",
        "        # Create positive samples tensor\n",
        "        pos_samples = torch.tensor(batch_triples, dtype=torch.long).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Create negative samples by corrupting either head or tail\n",
        "        neg_samples = []\n",
        "        for h, r, t in batch_triples:\n",
        "            for _ in range(neg_ratio):\n",
        "                if random.random() < 0.5:  # Corrupt head\n",
        "                    h_corrupt = random.randint(0, entity_count - 1)\n",
        "                    while h_corrupt == h:\n",
        "                        h_corrupt = random.randint(0, entity_count - 1)\n",
        "                    neg_samples.append((h_corrupt, r, t))\n",
        "                else:  # Corrupt tail\n",
        "                    t_corrupt = random.randint(0, entity_count - 1)\n",
        "                    while t_corrupt == t:\n",
        "                        t_corrupt = random.randint(0, entity_count - 1)\n",
        "                    neg_samples.append((h, r, t_corrupt))\n",
        "\n",
        "        neg_samples = torch.tensor(neg_samples, dtype=torch.long).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        return pos_samples, neg_samples\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJrjzAt0umpS",
        "outputId": "55ba328a-ec42-41ee-c8a1-7b52a6992bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model.py : Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "5PpMGx9oWNGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a file contains the `TransEModel` class, which encapsulates:\n",
        "- Initialization of entity and relation embeddings\n",
        "- L2 distance computation (used as a scoring function)\n",
        "- A forward pass to compute triple scores\n",
        "- Normalization of embeddings\n"
      ],
      "metadata": {
        "id": "80wu-gMvbryn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfpcOUc4tmsV",
        "outputId": "56f5ac1c-5a13-4516-a508-40076a9af81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "import torch # import torch module\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class TransEModel(nn.Module):\n",
        "    \"\"\"TransE Knowledge Graph Embedding Model\"\"\"\n",
        "\n",
        "    def __init__(self, entity_count: int, relation_count: int, embedding_dim: int = 100, device: str = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Initialize the TransE model\n",
        "\n",
        "        Args:\n",
        "            entity_count: Number of entities in the knowledge graph\n",
        "            relation_count: Number of relations in the knowledge graph\n",
        "            embedding_dim: Dimension of the embedding vectors\n",
        "            device: Device to run the model on (cuda or cpu)\n",
        "        \"\"\"\n",
        "        super(TransEModel, self).__init__()\n",
        "\n",
        "        self.entity_count = entity_count\n",
        "        self.relation_count = relation_count\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize embeddings with better scaling\n",
        "        self.entity_embeddings = nn.Embedding(entity_count, embedding_dim).to(device)\n",
        "        self.relation_embeddings = nn.Embedding(relation_count, embedding_dim).to(device)\n",
        "\n",
        "        torch.nn.init.uniform_(\n",
        "            self.entity_embeddings.weight.data,\n",
        "            -6/np.sqrt(embedding_dim),\n",
        "            6/np.sqrt(embedding_dim)\n",
        "        )\n",
        "        torch.nn.init.uniform_(\n",
        "            self.relation_embeddings.weight.data,\n",
        "            -6/np.sqrt(embedding_dim),\n",
        "            6/np.sqrt(embedding_dim)\n",
        "        )\n",
        "\n",
        "        # Save initial embeddings for later comparison\n",
        "        self.initial_entity_embeddings = self.entity_embeddings.weight.data.clone().cpu().numpy()\n",
        "        self.initial_relation_embeddings = self.relation_embeddings.weight.data.clone().cpu().numpy()\n",
        "\n",
        "    def l2_distance(self, head, relation, tail):\n",
        "        \"\"\"Compute L2 distance for TransE: ||h + r - t||_2.\"\"\"\n",
        "        return torch.norm(head + relation - tail, p=2, dim=1)\n",
        "\n",
        "    def forward(self, triples, normalize=True):\n",
        "        \"\"\"\n",
        "        Forward pass to compute scores for triples\n",
        "\n",
        "        Args:\n",
        "            triples: Tensor of shape (batch_size, 3) containing (head, relation, tail) triples\n",
        "            normalize: Whether to normalize embeddings\n",
        "\n",
        "        Returns:\n",
        "            Tensor of scores (L2 distances) for each triple\n",
        "        \"\"\"\n",
        "        heads = self.entity_embeddings(triples[:, 0])\n",
        "        relations = self.relation_embeddings(triples[:, 1])\n",
        "        tails = self.entity_embeddings(triples[:, 2])\n",
        "\n",
        "        # Optional normalization\n",
        "        if normalize:\n",
        "            heads = F.normalize(heads, p=2, dim=1)\n",
        "            relations = F.normalize(relations, p=2, dim=1)\n",
        "            tails = F.normalize(tails, p=2, dim=1)\n",
        "\n",
        "        return self.l2_distance(heads, relations, tails)\n",
        "\n",
        "    def normalize_embeddings(self):\n",
        "        \"\"\"Normalize embeddings to unit length\"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.entity_embeddings.weight.data = F.normalize(self.entity_embeddings.weight.data, p=2, dim=1)\n",
        "            self.relation_embeddings.weight.data = F.normalize(self.relation_embeddings.weight.data, p=2, dim=1)\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"Get all parameters of the model\"\"\"\n",
        "        return list(self.entity_embeddings.parameters()) + list(self.relation_embeddings.parameters())\n",
        "\n",
        "    def get_state_dict(self):\n",
        "        \"\"\"Get state dict for saving the model\"\"\"\n",
        "        return {\n",
        "            'entity_embeddings': self.entity_embeddings.state_dict(),\n",
        "            'relation_embeddings': self.relation_embeddings.state_dict()\n",
        "        }\n",
        "\n",
        "    def load_state_dict_from_dict(self, state_dict):\n",
        "        \"\"\"Load state dict from dictionary\"\"\"\n",
        "        self.entity_embeddings.load_state_dict(state_dict['entity_embeddings'])\n",
        "        self.relation_embeddings.load_state_dict(state_dict['relation_embeddings'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define trainer.py: TransE Model Trainer with Differential Privacy\n"
      ],
      "metadata": {
        "id": "_vjsUPjEWXOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script defines the `TransETrainer` class, which is responsible for:\n",
        "\n",
        "- Managing the complete training process of the TransE model.\n",
        "- Supporting differential privacy through per-sample gradient clipping and Gaussian noise injection.\n",
        "- Alternate between training on confidential and unrestricted triples based on dataset balance.\n",
        "- Apply a custom margin-based ranking loss with optional L2 regularization.\n",
        "- Monitoring differential privacy guarantees using Opacus's RDPAccountant.\n",
        "- Evaluate model performance using standard filtered ranking metrics (MR, MRR, Hits@K).\n",
        "- Incorporate early stopping based on validation performance or privacy budget (ε) exhaustion.\n",
        "- Normalize embeddings after updates to ensure stable optimization in the TransE space."
      ],
      "metadata": {
        "id": "ZFg9GIPDcI55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile trainer.py\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.optim import AdamW\n",
        "from opacus.accountants import RDPAccountant\n",
        "import torch\n",
        "from model import TransEModel\n",
        "from data import KGDataHandler\n",
        "\n",
        "class TransETrainer:\n",
        "    \"\"\"TransE Model Trainer with Differential Privacy Support\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: TransEModel,\n",
        "        data_handler: KGDataHandler,\n",
        "        learning_rate: float = 0.005,\n",
        "        noise_multiplier: float = 0.7,\n",
        "        batch_size: int = 256,\n",
        "        norm_clipping: float = 1.0,\n",
        "        margin: float = 0.5,\n",
        "        epochs: int = 300,\n",
        "        reg_lambda: float = 1e-5,\n",
        "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the trainer\n",
        "\n",
        "        Args:\n",
        "            model: TransE model\n",
        "            data_handler: Knowledge graph data handler\n",
        "            learning_rate: Step size for optimizer updates\n",
        "            noise_multiplier: Amount of noise added to gradients for differential privacy\n",
        "            batch_size: Number of samples per training batch\n",
        "            norm_clipping: Maximum L2 norm of per-sample gradients (before adding noise)\n",
        "            margin: Margin used in ranking loss to separate positive/negative triples\n",
        "            epochs: Total number of training iterations over the dataset\n",
        "            reg_lambda: Regularization parameter\n",
        "            device: Device to run the model on (cuda or cpu)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.data_handler = data_handler\n",
        "        self.learning_rate = learning_rate\n",
        "        self.noise_multiplier = noise_multiplier\n",
        "        self.batch_size = batch_size\n",
        "        self.norm_clipping = norm_clipping\n",
        "        self.margin = margin\n",
        "        self.epochs = epochs\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize optimizer and scheduler\n",
        "        self.parameters = self.model.get_parameters()\n",
        "        self.optimizer = AdamW(self.parameters, lr=learning_rate, weight_decay=1e-5)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "        )\n",
        "\n",
        "        # Privacy accountant\n",
        "        self.accountant = RDPAccountant()\n",
        "\n",
        "    def loss_function(self, pos_scores, neg_scores):\n",
        "        \"\"\"\n",
        "        Margin-based ranking loss for TransE with regularization\n",
        "\n",
        "        Args:\n",
        "            pos_scores: Scores for positive samples\n",
        "            neg_scores: Scores for negative samples\n",
        "\n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        # Reshape to ensure compatibility\n",
        "        pos_expanded = pos_scores.unsqueeze(1).expand(-1, neg_scores.size(0) // pos_scores.size(0))\n",
        "        pos_expanded = pos_expanded.reshape(-1)\n",
        "\n",
        "        ranking_loss = torch.mean(torch.relu(pos_expanded - neg_scores + self.margin))\n",
        "\n",
        "        # Add regularization term\n",
        "        if self.reg_lambda > 0:\n",
        "            # L2 regularization on parameters\n",
        "            reg_loss = 0\n",
        "            for param in self.parameters:\n",
        "                reg_loss += torch.norm(param, p=2)\n",
        "            return ranking_loss + self.reg_lambda * reg_loss\n",
        "\n",
        "        return ranking_loss\n",
        "\n",
        "    def optimize_confidential(self, triples):\n",
        "        \"\"\"\n",
        "        Optimize parameters for confidential statements with improved differential privacy\n",
        "\n",
        "        Args:\n",
        "            triples: List of triples [(head, relation, tail), ...]\n",
        "\n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        # Get positive and negative samples\n",
        "        pos_samples, neg_samples = self.data_handler.get_positive_and_negative_samples(\n",
        "            triples, self.batch_size, neg_ratio=5\n",
        "        )\n",
        "\n",
        "        # Forward pass\n",
        "        pos_score = self.model.forward(pos_samples, normalize=False)\n",
        "        neg_score = self.model.forward(neg_samples, normalize=False)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.loss_function(pos_score, neg_score)\n",
        "\n",
        "        # Backward pass to get gradients\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Improved gradient clipping using global norm\n",
        "        total_norm = 0\n",
        "        for param in self.parameters:\n",
        "            if param.grad is not None:\n",
        "                param_norm = param.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "        total_norm = total_norm ** 0.5\n",
        "\n",
        "        # Clip gradients globally (more stable than per-parameter)\n",
        "        clip_coef = min(1.0, self.norm_clipping / (total_norm + 1e-6))\n",
        "        for param in self.parameters:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.mul_(clip_coef)\n",
        "\n",
        "        # Add calibrated noise for differential privacy\n",
        "        for param in self.parameters:\n",
        "            if param.grad is not None:\n",
        "                noise = torch.normal(\n",
        "                    mean=0.0,\n",
        "                    std=self.noise_multiplier * self.norm_clipping / self.batch_size**0.5,\n",
        "                    size=param.grad.shape,\n",
        "                    device=self.device\n",
        "                )\n",
        "                param.grad.data += noise\n",
        "\n",
        "        # Update parameters\n",
        "        self.optimizer.step()\n",
        "        sample_rate = self.batch_size / len(self.data_handler.confidential_triples)\n",
        "\n",
        "        self.accountant.step(noise_multiplier=self.noise_multiplier, sample_rate=sample_rate)\n",
        "\n",
        "        # Normalize embeddings after update\n",
        "        self.model.normalize_embeddings()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def optimize_unrestricted(self, triples):\n",
        "        \"\"\"\n",
        "        Optimize parameters for unrestricted statements without differential privacy\n",
        "\n",
        "        Args:\n",
        "            triples: List of triples [(head, relation, tail), ...]\n",
        "\n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        # Get positive and negative samples\n",
        "        pos_samples, neg_samples = self.data_handler.get_positive_and_negative_samples(\n",
        "            triples, self.batch_size, neg_ratio=10\n",
        "        )\n",
        "\n",
        "        # Forward pass\n",
        "        pos_score = self.model.forward(pos_samples, normalize=False)\n",
        "        neg_score = self.model.forward(neg_samples, normalize=False)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.loss_function(pos_score, neg_score)\n",
        "\n",
        "        # Backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Normalize embeddings after update\n",
        "        self.model.normalize_embeddings()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def evaluate_model(self, test_triples, all_triples, batch_size=128, k=10):\n",
        "        \"\"\"\n",
        "        Evaluate model using filtered setting with batch processing\n",
        "\n",
        "        Args:\n",
        "            test_triples: List of test triples\n",
        "            all_triples: List of all triples (train + test)\n",
        "            batch_size: Batch size for evaluation\n",
        "            k: K for Hits@K metric\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "        # Convert all_triples to a dictionary for O(1) lookup\n",
        "        head_filter = {}\n",
        "        tail_filter = {}\n",
        "\n",
        "        for h, r, t in all_triples:\n",
        "            if (h, r) not in tail_filter:\n",
        "                tail_filter[(h, r)] = []\n",
        "            tail_filter[(h, r)].append(t)\n",
        "\n",
        "            if (r, t) not in head_filter:\n",
        "                head_filter[(r, t)] = []\n",
        "            head_filter[(r, t)].append(h)\n",
        "\n",
        "        head_ranks = []\n",
        "        tail_ranks = []\n",
        "\n",
        "        test_batches = [test_triples[i:i+batch_size] for i in range(0, len(test_triples), batch_size)]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_batches:\n",
        "                for h, r, t in batch:\n",
        "                    # Corrupt head\n",
        "                    head_candidates = []\n",
        "                    head_ids = []\n",
        "\n",
        "                    for e in range(self.data_handler.entity_count):\n",
        "                        if e != h and (e not in head_filter.get((r, t), [])):\n",
        "                            head_candidates.append((e, r, t))\n",
        "                            head_ids.append(e)\n",
        "\n",
        "                    # Add true triple\n",
        "                    head_candidates.append((h, r, t))\n",
        "                    head_ids.append(h)\n",
        "\n",
        "                    # Get scores for head batch\n",
        "                    if head_candidates:\n",
        "                        head_tensors = torch.tensor(head_candidates, device=self.device)\n",
        "                        head_scores = self.model.forward(head_tensors).cpu().numpy()\n",
        "\n",
        "                        # In TransE, LOWER scores are better (distance-based)\n",
        "                        true_idx = head_ids.index(h)\n",
        "                        true_score = head_scores[true_idx]\n",
        "                        # Count entities with better (lower) scores than the true entity\n",
        "                        head_rank = 1 + np.sum(head_scores < true_score)\n",
        "                        head_ranks.append(head_rank)\n",
        "\n",
        "                    # Corrupt tail\n",
        "                    tail_candidates = []\n",
        "                    tail_ids = []\n",
        "\n",
        "                    for e in range(self.data_handler.entity_count):\n",
        "                        if e != t and (e not in tail_filter.get((h, r), [])):\n",
        "                            tail_candidates.append((h, r, e))\n",
        "                            tail_ids.append(e)\n",
        "\n",
        "                    # Add true triple\n",
        "                    tail_candidates.append((h, r, t))\n",
        "                    tail_ids.append(t)\n",
        "\n",
        "                    # Get scores for tail batch\n",
        "                    if tail_candidates:\n",
        "                        tail_tensors = torch.tensor(tail_candidates, device=self.device)\n",
        "                        tail_scores = self.model.forward(tail_tensors).cpu().numpy()\n",
        "\n",
        "                        # Find rank of true triple (lower is better)\n",
        "                        true_idx = tail_ids.index(t)\n",
        "                        true_score = tail_scores[true_idx]\n",
        "                        # Count entities with better (lower) scores than the true entity\n",
        "                        tail_rank = 1 + np.sum(tail_scores < true_score)\n",
        "                        tail_ranks.append(tail_rank)\n",
        "\n",
        "        # Calculate metrics\n",
        "        all_ranks = head_ranks + tail_ranks\n",
        "        mr = sum(all_ranks) / len(all_ranks) if all_ranks else 0\n",
        "        mrr = sum(1.0/r for r in all_ranks) / len(all_ranks) if all_ranks else 0\n",
        "        hits_at_1 = sum(1 for r in all_ranks if r <= 1) / len(all_ranks) if all_ranks else 0\n",
        "        hits_at_3 = sum(1 for r in all_ranks if r <= 3) / len(all_ranks) if all_ranks else 0\n",
        "        hits_at_k = sum(1 for r in all_ranks if r <= k) / len(all_ranks) if all_ranks else 0\n",
        "\n",
        "        return {\n",
        "            'MR': mr,\n",
        "            'MRR': mrr,\n",
        "            'Hits@1': hits_at_1,\n",
        "            'Hits@3': hits_at_3,\n",
        "            'Hits@10': hits_at_k\n",
        "        }\n",
        "\n",
        "    def train_with_early_stopping(self, patience=5):\n",
        "        \"\"\"\n",
        "        Train model with early stopping\n",
        "\n",
        "        Args:\n",
        "            patience: Number of evaluation rounds with no improvement before early stopping\n",
        "\n",
        "        Returns:\n",
        "            Best model state, training losses, epsilon values\n",
        "        \"\"\"\n",
        "        best_hits = 0\n",
        "        no_improve = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        mU = 0  # Counter for unrestricted optimization steps\n",
        "        mC = 0  # Counter for confidential optimization steps\n",
        "        losses = []\n",
        "        epsilon_values = []\n",
        "        all_metrics = []\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
        "            steps = 0\n",
        "            epoch_losses = []\n",
        "\n",
        "            while True:\n",
        "                # Stop if both datasets are exhausted\n",
        "                if (len(self.data_handler.unrestricted_triples) == 0 and\n",
        "                    len(self.data_handler.confidential_triples) == 0):\n",
        "                    break\n",
        "\n",
        "                # Decide which type of batch to sample (improved balance calculation)\n",
        "                if (len(self.data_handler.unrestricted_triples) == 0 or\n",
        "                    (mC < mU * len(self.data_handler.confidential_triples) /\n",
        "                     len(self.data_handler.unrestricted_triples))):\n",
        "                    batch_type = \"confidential\"\n",
        "                    loss = self.optimize_confidential(self.data_handler.confidential_triples)\n",
        "                    mC += 1\n",
        "                else:\n",
        "                    batch_type = \"unrestricted\"\n",
        "                    loss = self.optimize_unrestricted(self.data_handler.unrestricted_triples)\n",
        "                    mU += 1\n",
        "\n",
        "                epoch_losses.append(loss)\n",
        "                steps += 1\n",
        "\n",
        "                if steps >= ((len(self.data_handler.unrestricted_triples) +\n",
        "                              len(self.data_handler.confidential_triples)) // self.batch_size):\n",
        "                    break\n",
        "\n",
        "            avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0\n",
        "            print(f\"  Epoch {epoch+1}: mU={mU}, mC={mC}, avg_loss={avg_loss:.4f}\")\n",
        "            losses.append(avg_loss)\n",
        "\n",
        "            # Update learning rate based on average loss\n",
        "            self.scheduler.step(avg_loss)\n",
        "\n",
        "            # Evaluate on validation set every 10 epochs\n",
        "            if epoch % 10 == 0:\n",
        "                val_sample = random.sample(\n",
        "                    self.data_handler.test_triples,\n",
        "                    min(500, len(self.data_handler.test_triples))\n",
        "                )\n",
        "                metrics = self.evaluate_model(val_sample, self.data_handler.all_triples)\n",
        "                print(f\"  Validation: MR={metrics['MR']:.2f}, MRR={metrics['MRR']:.4f}, \" +\n",
        "                      f\"Hits@10={metrics['Hits@10']:.4f}\")\n",
        "                metrics[\"epoch\"] = epoch\n",
        "                all_metrics.append(metrics)\n",
        "                # Check for improvement\n",
        "                current_hits = metrics['Hits@10']\n",
        "                if current_hits > best_hits:\n",
        "                    best_hits = current_hits\n",
        "                    no_improve = 0\n",
        "                    # Save best model\n",
        "                    best_model_state = {\n",
        "                        'entity_embeddings': self.model.entity_embeddings.state_dict(),\n",
        "                        'relation_embeddings': self.model.relation_embeddings.state_dict(),\n",
        "                        'epoch': epoch,\n",
        "                        'metrics': metrics\n",
        "                    }\n",
        "                else:\n",
        "                    no_improve += 1\n",
        "\n",
        "                if no_improve >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    # Restore best model\n",
        "                    if best_model_state:\n",
        "                        self.model.load_state_dict_from_dict(best_model_state)\n",
        "                    break\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                eps = self.accountant.get_epsilon(delta=1e-5)\n",
        "                print(f\"  Current privacy guarantee: (ε = {eps:.2f}, δ = 1e-5)\")\n",
        "                epsilon_values.append((epoch, eps))\n",
        "                if eps > 10:\n",
        "                    print(\"Epsilon exceed, stopping at epoch \", (epoch + 1))\n",
        "                    if best_model_state:\n",
        "                        self.model.load_state_dict_from_dict(best_model_state)\n",
        "                    return best_model_state, losses, epsilon_values, all_metrics\n",
        "\n",
        "        return best_model_state, losses, epsilon_values, all_metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4l3VRPYvI5X",
        "outputId": "57c3a354-5965-466f-917a-c3809eeff0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting trainer.py\n"
          ]
        }
      ]
    }
  ]
}